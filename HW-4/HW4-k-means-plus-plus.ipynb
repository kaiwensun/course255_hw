{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name: Kaiwen Sun\n",
    "# Email: kas003@eng.ucsd.edu\n",
    "# PID: A53091621"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means++\n",
    "\n",
    "In this notebook, we are going to implement [k-means++](https://en.wikipedia.org/wiki/K-means%2B%2B) algorithm with multiple initial sets. The original k-means++ algorithm will just sample one set of initial centroid points and iterate until the result converges. The only difference in this implementation is that we will sample `RUNS` sets of initial centroid points and update them in parallel. The procedure will finish when all centroid sets are converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Definition of some global parameters.\n",
    "K = 5  # Number of centroids\n",
    "RUNS = 25  # Number of K-means runs that are executed in parallel. Equivalently, number of sets of initial points\n",
    "RANDOM_SEED = 60295531\n",
    "converge_dist = 0.1 # The K-means algorithm is terminated when the change in the location \n",
    "                    # of the centroids is smaller than 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "from numpy.linalg import norm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def print_log(s):\n",
    "    sys.stdout.write(s + \"\\n\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def parse_data(row):\n",
    "    '''\n",
    "    Parse each pandas row into a tuple of (station_name, feature_vec),\n",
    "    where feature_vec is the concatenation of the projection vectors\n",
    "    of TAVG, TRANGE, and SNWD.\n",
    "    '''\n",
    "    return (row[0],\n",
    "            np.concatenate([row[1], row[2], row[3]]))\n",
    "\n",
    "\n",
    "def compute_entropy(d):\n",
    "    '''\n",
    "    Compute the entropy given the frequency vector `d`\n",
    "    '''\n",
    "    d = np.array(d)\n",
    "    d = 1.0 * d / d.sum()\n",
    "    return -np.sum(d * np.log2(d))\n",
    "\n",
    "\n",
    "def choice(p):\n",
    "    '''\n",
    "    Generates a random sample from [0, len(p)),\n",
    "    where p[i] is the probability associated with i. \n",
    "    '''\n",
    "    random = np.random.random()\n",
    "    r = 0.0\n",
    "    for idx in range(len(p)):\n",
    "        r = r + p[idx]\n",
    "        if r > random:\n",
    "            return idx\n",
    "    assert(False)\n",
    "\n",
    "\n",
    "def kmeans_init(rdd, K, RUNS, seed):\n",
    "    '''\n",
    "    Select `RUNS` sets of initial points for `K`-means++\n",
    "    '''\n",
    "    # the `centers` variable is what we want to return\n",
    "    n_data = rdd.count()\n",
    "    shape = rdd.take(1)[0][1].shape[0]\n",
    "    centers = np.zeros((RUNS, K, shape))\n",
    "\n",
    "    def update_dist(vec, dist, k):\n",
    "        new_dist = norm(vec - centers[:, k], axis=1)**2\n",
    "        return np.min([dist, new_dist], axis=0)\n",
    "\n",
    "\n",
    "    # The second element `dist` in the tuple below is the closest distance from\n",
    "    # each data point to the selected points in the initial set, where `dist[i]`\n",
    "    # is the closest distance to the points in the i-th initial set.\n",
    "    data = rdd.map(lambda p: (p, [np.inf] * RUNS)) \\\n",
    "              .cache()\n",
    "\n",
    "    # Collect the feature vectors of all data points beforehand, might be\n",
    "    # useful in the following for-loop\n",
    "    local_data = rdd.map(lambda (name, vec): vec).collect()\n",
    "\n",
    "    # Randomly select the first point for every run of k-means++,\n",
    "    # i.e. randomly select `RUNS` points and add it to the `centers` variable\n",
    "    sample = [local_data[k] for k in np.random.randint(0, len(local_data), RUNS)]\n",
    "    centers[:, 0] = sample\n",
    "\n",
    "    for idx in range(K - 1):\n",
    "        data1 = data\n",
    "        #找出来每个vec距离到idx个种子点中最近的那个种子点的距离\n",
    "        for k in range(idx+1):\n",
    "            data1 = data1.map(lambda ((name,vec),dist):((name,vec),update_dist(vec,dist,k))).cache()\n",
    "        \n",
    "        #内循环结束后，每一行对应一个数据点，是：((name,vec),[在25个宇宙里到最近的黑洞的距离])\n",
    "        \n",
    "        #把距离normalize成和为1的概率\n",
    "        #data2里有n行，每行对应一个星球（数据点）。每行的内容是一个长度为25的向量，表示这个星球在各个宇宙中距离最近黑洞的距离（的平方）。\n",
    "        data2 = data1.map(lambda ((name,vec),dist):dist).cache()\n",
    "        #summation是一个长度为25的向量，表示每个宇宙中dist的和\n",
    "        summation = data2.reduce(lambda distsq1,distsq2:distsq1+distsq2)\n",
    "        \n",
    "        #data3是个n乘25的矩阵，每列对应一个宇宙，每列的和都是1，表示一个dist归一化了的宇宙\n",
    "        data3 = np.array(data2.map(lambda distsq:distsq/summation).collect())\n",
    "        \n",
    "        #index 是一个向量，长度为25，表示每个宇宙中新变成（第k个）黑洞的星球的index\n",
    "        index = np.apply_along_axis(func1d=choice,axis=0,arr=data3)\n",
    "        \n",
    "        #根据星球坐标表(local_data，各个宇宙共享)，把各个宇宙中新黑洞的index翻译成新黑洞的坐标\n",
    "        centers[:,idx+1]=np.array([local_data[i] for i in index])\n",
    "        \n",
    "        ##############################################################################\n",
    "        # Insert your code here:\n",
    "        ##############################################################################\n",
    "        # In each iteration, you nee(da,di) (da,update_dist(local_data,di,idx))to select one point for each set\n",
    "        # of initial points (so select `RUNS` points in total).\n",
    "        # For each data point x, let D_i(x) be the distance between x and\n",
    "        # the nearest center that has already been added to the i-th set.\n",
    "        # Choose a new data point for i-th set using a weighted probability\n",
    "        # where point x is chosen with probability proportional to D_i(x)^2\n",
    "        ##############################################################################\n",
    "        #data = rdd.map(lambda p: (p, [np.inf] * RUNS)) \\\n",
    "        #      .cache()\n",
    "\n",
    "    return centers\n",
    "\n",
    "\n",
    "def get_closest(p, centers):\n",
    "    '''\n",
    "    Return the indices the nearest centroids of `p`.\n",
    "    `centers` contains sets of centroids, where `centers[i]` is\n",
    "    the i-th set of centroids.\n",
    "    '''\n",
    "    best = [0] * len(centers)\n",
    "    closest = [np.inf] * len(centers)\n",
    "    for idx in range(len(centers)):\n",
    "        for j in range(len(centers[0])):\n",
    "            temp_dist = norm(p - centers[idx][j])\n",
    "            if temp_dist < closest[idx]:\n",
    "                closest[idx] = temp_dist\n",
    "                best[idx] = j\n",
    "    return best\n",
    "\n",
    "\n",
    "def kmeans(rdd, K, RUNS, converge_dist, seed):\n",
    "    '''\n",
    "    Run K-means++ algorithm on `rdd`, where `RUNS` is the number of\n",
    "    initial sets to use.\n",
    "    '''\n",
    "    #k_points.shape = (RUNS,K,9) = (25,5,9)。25个平行宇宙，每个宇宙里选出了5个黑洞\n",
    "    k_points = kmeans_init(rdd, K, RUNS, seed)\n",
    "    \n",
    "    #print \"k_points=\",k_points\n",
    "    #print \"k_points.shape=\",k_points.shape\n",
    "    \n",
    "    print_log(\"Initialized.\")\n",
    "    temp_dist = 1.0\n",
    "\n",
    "    iters = 0\n",
    "    st = time.time()\n",
    "    \n",
    "    #rdd的每一行是(name,vec),其中vec是星球的坐标（长度为9）\n",
    "    rdd = rdd.cache()\n",
    "    shape = rdd.take(1)[0][1].shape[0]\n",
    "    \n",
    "    while temp_dist > converge_dist:\n",
    "        new_points = np.zeros((RUNS, K, shape))\n",
    "        \n",
    "        #data1有n行，每行对应一个星球，每一行是(星球坐标，星球在25个宇宙里所附属于的黑洞id)\n",
    "        #data1 = rdd.map(lambda (name,vec):(vec,get_closest(vec,k_points)))\n",
    "        \n",
    "        #data2有n行，每行对应一个星球，每一行是(星球坐标，[(0,该星球在第0个宇宙里所属的黑洞id),...,(24,该星球在第24个宇宙里所属的黑洞id)])\n",
    "        #data2 = data1.map(lambda (vec,ids):(vec,zip(range(25),ids)))\n",
    "        \n",
    "        #data3的每一项是一个key-value pair。key是(宇宙id,黑洞id)，value是(一个附属于这个宇宙中的这个黑洞上的星球的坐标,1)\n",
    "        #data3 = data2.flatMap(lambda (vec,lst_univid_bhid):[(univid_bhid,(vec,1)) for univid_bhid in lst_univid_bhid])\n",
    "        \n",
    "        #compress computations above to one line\n",
    "        data3 = rdd.flatMap(lambda (name,vec):[((univid,bhid),(vec,1)) for (univid,bhid) in zip(range(25),get_closest(vec,k_points))])\n",
    "        \n",
    "        #daat4一共有25*5项，每一项是一个key-value pair。key是(宇宙id,黑洞id)，value是(附属于这个宇宙中的这个黑洞上的星球距离黑洞的距离之和,附属的星球的个数)\n",
    "        data4 = data3.reduceByKey(lambda (vec1,cnt1),(vec2,cnt2):(vec1+vec2,cnt1+cnt2))\n",
    "        \n",
    "        #data5一共有25×5项，每一项是一个key-value pair。key是(宇宙id，黑洞id)，value是这个宇宙里的这个黑洞的新坐标\n",
    "        data5 = data4.map(lambda ((univid,bhid),(vecsum,cnt)):((univid,bhid),vecsum/cnt)).collect()\n",
    "\n",
    "        for ((univid,bhid),newvec) in data5:\n",
    "            new_points[univid][bhid]=newvec\n",
    "        \n",
    "        \n",
    "        ##############################################################################\n",
    "        # INSERT YOUR CODE HERE\n",
    "        ##############################################################################\n",
    "        \n",
    "        # Update all `RUNS` sets of centroids using standard k-means algorithm\n",
    "        # Outline:\n",
    "        #   - For each point x, select its nearest centroid in i-th centroids set\n",
    "        #   - Average all points that are assigned to the same centroid\n",
    "        #   - Update the centroid with the average of all points that are assigned to it\n",
    "        \n",
    "        # Insert your code here\n",
    "\n",
    "        # You can modify this statement as long as `temp_dist` equals to\n",
    "        # max( sum( l2_norm of the movement of j-th centroid in each centroids set ))\n",
    "        ##############################################################################\n",
    "\n",
    "        temp_dist = np.max([\n",
    "                np.sum([norm(k_points[idx][j] - new_points[(idx, j)]) for j in range(K)])\n",
    "                    for idx in range(RUNS)])\n",
    "\n",
    "        iters = iters + 1\n",
    "        if iters % 5 == 0:\n",
    "            print_log(\"Iteration %d max shift: %.2f (time: %.2f)\" %\n",
    "                      (iters, temp_dist, time.time() - st))\n",
    "            st = time.time()\n",
    "\n",
    "        # update old centroids\n",
    "        # You modify this for-loop to meet your need\n",
    "        #for ((idx, j), p) in new_points.items():\n",
    "        #    k_points[idx][j] = p\n",
    "        k_points = new_points\n",
    "        \n",
    "        #print_log(\"iters=%d\"%(iters))\n",
    "\n",
    "    return k_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'USC00044534', array([  3.04796236e+03,   1.97434852e+03,   1.50560792e+02,\n",
       "          -2.90363288e+03,  -2.36907268e+02,   1.47021791e+02,\n",
       "           1.91503001e-01,   1.87262808e-01,  -4.01379553e-02]))]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read data\n",
    "data = pickle.load(open(\"../Data/Weather/stations_projections.pickle\", \"rb\"))\n",
    "rdd = sc.parallelize([parse_data(row[1]) for row in data.iterrows()])\n",
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized.\n",
      "Iteration 5 max shift: 3211.03 (time: 45.67)\n",
      "Iteration 10 max shift: 1928.05 (time: 49.15)\n",
      "Iteration 15 max shift: 693.41 (time: 55.62)\n",
      "Iteration 20 max shift: 348.29 (time: 62.97)\n",
      "Iteration 25 max shift: 235.29 (time: 62.22)\n",
      "Iteration 30 max shift: 185.35 (time: 66.32)\n",
      "Iteration 35 max shift: 51.71 (time: 63.33)\n",
      "Iteration 40 max shift: 45.07 (time: 60.06)\n",
      "Iteration 45 max shift: 26.03 (time: 61.24)\n",
      "Iteration 50 max shift: 15.59 (time: 62.66)\n",
      "Iteration 55 max shift: 0.85 (time: 58.06)\n",
      "Time takes to converge: 679.807723999\n"
     ]
    }
   ],
   "source": [
    "# main code\n",
    "\n",
    "import time\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "centroids = kmeans(rdd, K, RUNS, converge_dist, np.random.randint(1000))\n",
    "group = rdd.mapValues(lambda p: get_closest(p, centroids)) \\\n",
    "           .collect()\n",
    "\n",
    "print \"Time takes to converge:\", time.time() - st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify your results\n",
    "Verify your results by computing the objective function of the k-means clustering problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_cost(rdd, centers):\n",
    "    '''\n",
    "    Compute the square of l2 norm from each data point in `rdd`\n",
    "    to the centroids in `centers`\n",
    "    '''\n",
    "    def _get_cost(p, centers):\n",
    "        best = [0] * len(centers)\n",
    "        closest = [np.inf] * len(centers)\n",
    "        for idx in range(len(centers)):\n",
    "            for j in range(len(centers[0])):\n",
    "                temp_dist = norm(p - centers[idx][j])\n",
    "                if temp_dist < closest[idx]:\n",
    "                    closest[idx] = temp_dist\n",
    "                    best[idx] = j\n",
    "        return np.array(closest)**2\n",
    "    \n",
    "    cost = rdd.map(lambda (name, v): _get_cost(v, centroids)).collect()\n",
    "    return np.array(cost).sum(axis=0)\n",
    "\n",
    "cost = get_cost(rdd, centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.8254902123 33.7575332525 33.7790236109\n"
     ]
    }
   ],
   "source": [
    "log2 = np.log2\n",
    "\n",
    "print log2(np.max(cost)), log2(np.min(cost)), log2(np.mean(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the increase of entropy after multiple runs of k-means++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entropy = []\n",
    "\n",
    "for i in range(RUNS):\n",
    "    count = {}\n",
    "    for g, sig in group:\n",
    "        _s = ','.join(map(str, sig[:(i + 1)]))\n",
    "        count[_s] = count.get(_s, 0) + 1\n",
    "    entropy.append(compute_entropy(count.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Remove this cell before submitting to PyBolt (PyBolt does not fully support matplotlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.8799771938634473"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEPCAYAAABCyrPIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAF1pJREFUeJzt3X2QHHWZwPHvkhfzsoHwmvCSuBBCQCVHQBEkkkFRAUuJ\nqHhyyGtBoSiIoBTxJXtFnUfhoZyUeDkhy5uEOxBQKIzAHYOJQiAhr7wkJILAsUkIJGQDCQlh7o9f\nD7vZzM7ObranZ7q/n6qp6Z7u7X7SDP3M77VBkiRJkiRJkiRJkiRJkiRJStQo4BHgaWAJcFGJfU4G\nFgLzgXnAp6oWnSSpakYCh0XLjcBS4JBO+wztsHwosLwKcUmSStgpxmOvBBZEyxuAZ4F9Ou3zVofl\nRmBNjPFIkmpAE/B3wk2/s8mEZLEOOLKKMUmSqqwRmEu48ZfzSUK1kiQpAf1jPv4A4HfAbcC93ew7\nK4pnd+D1jhvGjBlTWLFiRSwBSlKKrQAOrHTnONsQGoAbgWeAa7vYZ0y0H8Dh0fvrnXdasWIFhULB\nV6HA1KlTE4+hVl5eC6+F16L8K7rHVizOEsIxwOnAIkK3UoApwOhoeRrwZeAMYAuh4fkfY4xHklRG\nnAlhNt2XQK6OXpJ6oVCANWugrQ1efTXpaEorFGDTJtiwof311ltdrxeX33mnd+d7/nmYM6dv/w1Z\nEXcbgvpYLpdLOoSakaVr0dYGy5aF19Kl274PGAANDTlmzEg6yq4NHgyNjTB0aHgvvjqu7733ttsG\nDoSGhu6P3dmiRTnGj+/7f0M9mjmzZ/v34nInohDVh6lK3ngDNm9OOorsWb++9E3/zTdh7Fg46CAY\nNy68F1+77pp01KpVDSGjVnyfNyFoG0uWwNSp8OCD4debqquxsf1G3/HGv+++sFOcXUCUSj1NCFYZ\nCQj1rs3N8PDD8P3vw623wpAhSUclqZr8zZFxL74I554LRx8NhxwCy5fDZZeZDKQsMiFk1KuvwoUX\nwhFHwD77hBLCj34Ew4YlHZmkpJgQMmb1arj0Ujj00FAKeO45uPJKGyYlmRAyY+1a+OEPQ7XQ5s2h\n8fhnP4M990w6Mkm1woSQcm+9FUoAY8eG0sFTT8F114U+35LUkb2MUu7ss8OIz8cfhwMrnuJKUhaZ\nEFLsvvtg/nxYtCiMFJWkchyYllJtbfDhD8NNN8GnfFK1lEmOVBYA3/1umO6gpSXpSCQlxZHK4okn\n4I474Omnk45EUj2xl1HKbNkC558P11wDu++edDSS6okJIWV+8QsYMQJOOy3pSCTVG9sQUmTFCvj4\nx0OV0QEHJB2NpKT1tA3BEkJKFArwzW/C5ZebDCT1jgkhJX77W3jtNbjkkqQjkVSvrDJKgTVr4CMf\ngfvvh49+NOloJNUKxyFk0Jlnhh5FP/950pFIqiWOQ8iYhx+GRx8Ns5dK0o6wDaGOvf02XHABXH99\neBavJO2IOBPCKOAR4GlgCXBRiX3+CVgILAL+AoyPMZ7UufLK0GZw0klJRyIpDeJsQxgZvRYAjcA8\nYDLwbId9jgaeAd4ETgCagaNKHMs2hE4WLYLjjw/vI0cmHY2kWlRL4xBWEpIBwAZCItin0z6PEZIB\nwBxgvxjjSY2tW+G88+CnPzUZSOo71WpDaAImEG76XTkXeKAq0dS566+HQYPgnHOSjkRSmlSjl1Ej\ncBdwMaGkUMpxwDnAMV0dpLm5+f3lXC5HLpfrswDrycsvwz//M8yeDTvZJUBSB/l8nnw+3+u/j3sc\nwgDgfuCPwLVd7DMeuJvQhrC8i31sQyBMT3HyyaEh+Sc/SToaSbWulsYhNAA3EhqNu0oGownJ4HS6\nTgaK3H03LF8Od96ZdCSS0ijOEsJE4M+ELqXFn/dTCEkAYBpwA/Al4KXosy3AkSWOlfkSwtatMGYM\n3HILHHts0tFIqgdOXZFSDz4IV1wB8+YlHYmkelFL3U7Vh1pa4Oyzk45CUppZQqgDa9fC/vvD3/4G\nu+2WdDSS6oUlhBS64w743OdMBpLiZUKoA9OnW10kKX4mhBq3ZAm0tsJnPpN0JJLSzoRQ41pa4Iwz\noF+/pCORlHY2KtewLVtgv/1g1iw46KCko5FUb2xUTpEHHoCxY00GkqrDhFDDWlqc0VRS9VhlVKNW\nrYJx48LspsOGJR2NpHpklVFK3HYbTJ5sMpBUPSaEGlQoOFWFpOozIdSguXNh40ZnNZVUXSaEGtTS\nAmedBQ310sIjKRXq5ZaTmUbljRvD2IP582H06O73l6Su2Khc5+69F444wmQgqfpMCDXGxmRJSbHK\nqIa89BJMmACvvAKDBycdjaR6Z5VRHbvlFjj1VJOBpGRYQqgRhUKYt+j22+HII5OORlIaWEKoU7Nm\nwaBB8LGPJR2JpKwyIdSI4lPRHHsgKSn1cvtJdZVRWxuMGgVLl8KIEUlHIyktaqnKaBTwCPA0sAS4\nqMQ+BwOPAZuAS2OMpabdeSdMmmQykJSs/jEeewtwCbAAaATmAQ8Bz3bY53XgO8DkGOOoeS0tcGlm\n06GkWhFnCWElIRkAbCAkgn067fMaMJeQPDLp+edh2TL4/OeTjkRS1lWrUbkJmADMqdL56sZNN8Hp\np8OAAUlHIinr4qwyKmoE7gIuJpQUeqW5ufn95VwuRy6X29G4Erd1K9x8M8ycmXQkktIgn8+Tz+d7\n/fdx9zIaANwP/BG4tsx+UwnJ4poutqeyl9HMmfDjH8OTTyYdiaQ0qqVeRg3AjcAzlE8GxX0zx4ns\nJNWSOG/EE4E/A4uA4s/7KUBxYudpwEjgSWBn4D2gDfgQ21ctpa6E8MYbcMAB8MILsOuuSUcjKY16\nWkKIsw1hNt2XQFYSxitkzowZcMIJJgNJtcOpKxLS0gLnnJN0FJLUrl7q7mOvMtq6FU45Bdati/U0\n75/rpZdCdVG/fvGfT1I21VKVUV1ZvRpmz4a7767O+Q44wGQgqbaYECIrV4aH20+alHQkkpQM2xAi\nra2w995JRyFJyTEhRFpbYeTIpKOQpOSYECIrV1pCkJRtJoSIVUaSss6EEDEhSMo6E0Jk5UrbECRl\nmwkhYglBUtY5UhkoFGDIEHjtNWhsjO00klRVtTT9dd1Yvx769zcZSMo2EwJWF0kSmBAAE4IkgQkB\ncJSyJIEJAXCUsiSBCQGwykiSwIQAWGUkSWBCAKwykiQwIQBWGUkSmBAAq4wkCZy6gnfegWHDYNMm\n2Mn0KClFamnqilHAI8DTwBLgoi72+yXwPLAQmBBjPCWtXAkjRpgMJKl/jMfeAlwCLAAagXnAQ8Cz\nHfY5CTgQGAt8HPg1cFSMMW3H9gNJCir5Xbx7L4+9kpAMADYQEsE+nfb5InBztDwHGA6M6OX5esXn\nIEhSUElCeBy4k/BrvrdtDk2E6qA5nT7fF3i5w/orwH69PEevWEKQpKCSKqNxwPHAOcB1wH8DLcCy\nCs/RCNwFXEwoKXTWOcmUbD1ubm5+fzmXy5HL5So8fXkmBElpkc/nyefzvf77nv7i/xRwGzCUUB10\nBfDXMvsPAO4H/ghcW2L7fwB54I5o/TlgErCq036x9TI6/3w4/HC44IJYDi9JiYmjl9EehF/384DL\ngG9Hn10K3F4uFuBG4BlKJwOAPwBnRMtHAevYPhnEyhKCJAWVVBn9lVAqOJlQx180l/ALvyvHAKcD\ni4D50WdTgNHR8jTgAULbxHLgLeDsSgPvKyYESQoqKUrsBLwH7Eyo32+LNaLSYqsy2ndfeOwxGD26\n+30lqZ7EUWV0BLA4ei0hDCD7aG+CqzXvvQerV9vtVJKgsiqj6cC3gFnR+sTos/FxBVUta9bALrvA\nwIFJRyJJyaukhPAu7ckAYHb0Wd1zUjtJaldJCeFRQgPwjGj9a9Fnh0frT8UQV1X4HARJaldJQjiM\n0Jg8NVpviNYPi9aPiyGuqrCHkSS1qyQh5OIOIilWGUlSu0raEIYDvyAMTJsHXAPsEmdQ1WKVkSS1\nqyQhTAfWA18FTiWMQ2iJM6hqscpIktpVUmU0Bjilw3ozYSxC3TMhSFK7SkoIG4FPdlifCLwdTzjV\n5bMQJKldJSWEC4BbaG83WAucGVtEVWQJQZLadZcQ+hEmqBtPe0J4M9aIqqStDQoFGDYs6UgkqTZ0\nlxC2EqqIGkhJIigqVhc19PYZcJKUMpVUGS0Afk94jGax7aAA3B1XUNVgdZEkbauShDAIeJ3wtLSO\nTAiSlCKVJIQbCBPadTQxhliqylHKkrStSrqd/rLCz+qKo5QlaVvlSghHA58A9gK+R/tTd4YReh/V\ntdZWGDcu6SgkqXaUSwgDab/5d+ycuR74SpxBVYNVRpK0rXIJ4dHodRPwYjWCqSarjCRpW5U0Kn8A\n+A3Q1GH/Atv3Oqor9jKSpG1VMixrEfBrwpPRtkafFQhTYVdLoVAo9NnBNm+GoUNh0yboV/etIZJU\nWkMYeVvx8NtKSghbCAkhNVavhj33NBlIUkeVdDu9D7gQ2BvYrcOrEtOBVcDiLrbvCtxDmE57DvDh\nCo+7Q6wukqTtVVJCOItQRXRZp8/3r+BvW4DrCLOlljKFUBX1JWAc8Cvg+AqOu0NMCJK0vUoSQtMO\nHH9WN39/CHBVtLw02ndP4LUdOGe3fA6CJG2vXJXRDzosf7XTtp/20fkX0v40tiOBDwL79dGxu2QJ\nQZK2V66E8HXg6mh5CmG206ITo8921FXAvwPzCe0M82nvybSN5ubm95dzuRy5XK7XJ21thfHje/3n\nklST8vk8+Xy+139frjvSfGBCieVS6+U0ERqmD61g3xei/TZ0+rxPu51OngxnnAGnnNL9vpJUr3ra\n7bSSXkZx2oUwRQbAeYSR0Z2TQZ+zykiStleuymg80BYtD+6wXFyvxAxgErAH8DIwFRgQbZsGfIgw\nNUYBWAKcW+Fxd4gJQZK2Vy8PkOyzKqNCAQYNgjffDO+SlFb1VmVUdW+8AUOGmAwkqbPMJQSriySp\ntEwmBAelSdL2MpcQfA6CJJWWuYRglZEklWZCkCQBGUwITmwnSaVlLiFYQpCk0kwIkiQggwnBKiNJ\nKi1TCeHtt2HzZhg+POlIJKn2ZCohFAelNdTLDE6SVEWZSghWF0lS1zKVEGxQlqSumRAkSUDGEoJV\nRpLUtUwlBEsIktQ1E4IkCchgQrDKSJJKy1RC8FkIktS1ehmiVSgUCjt0gHffhcGDYeNG6N+/j6KS\npBrWEEbhVnyfz0wJYfVq2H13k4EkdSUzCcEup5JUXtwJYTqwCljcxfY9gJnAAmAJcFZcgdjDSJLK\nizshtAAnlNn+bWA+cBiQA64BYqnUMSFIUnlxJ4RZwNoy21uBnaPlnYHXgXfjCMQqI0kqL+km1t8A\n/wu8CgwDTo3rRK2tcPDBcR1dkupf0glhCqH9IAeMAR4C/gFo67xjc3Pz+8u5XI5cLtejE7W2wnHH\n9TpOSap5+XyefD7f67+vxjiEJuA+4NAS2x4A/gX4S7T+P8DlwNxO++3wOIRPfAKuvhomTtyhw0hS\n3ai3cQjPAcdHyyOAccDf4jiRjcqSVF7cJYQZwCRC99JVwFRgQLRtWvR5CzCakJz+Fbi9xHF2qIRQ\nKIRRyq+/DkOH9vowklRXelpCyMTUFevWwejRsH59H0YkSTWu3qqMqsLqIknqnglBkgRkKCE4KE2S\nystEQvA5CJLUvUwkBKuMJKl7JgRJEpCRhODEdpLUvUwkBEsIktQ9E4IkCcjASOVNm2CXXcJ7Q738\nayWpDzhSuZOVK2HECJOBJHUn9QnB6iJJqkzqE4I9jCSpMqlPCJYQJKkyJgRJEpCBhGCVkSRVJvUJ\nwRKCJFXGhCBJAjKQEKwykqTK1MtwrV6NVN66FQYPhg0bYODAGKKSpBrmSOUO1qwJ01aYDCSpe6lO\nCLYfSFLlUp0QbD+QpMrFnRCmA6uAxV1svwyYH70WA+8Cw/vq5JYQJKlycSeEFuCEMtv/DZgQva4A\n8sC6vjq5CUGSKhd3QpgFrK1w39OAGX15cquMJKlytdKGMAT4HPC7vjyoJQRJqlz/pAOIfAGYTZnq\noubm5veXc7kcuVyu24OaECRlST6fJ5/P9/rvqzEwrQm4Dzi0zD73AP8F3NHF9l4NTDvwQHjgATjo\noB7/qSTVvXocmLYLcCzw+748aKFgCUGSeiLuKqMZwCRgD+BlYCowINo2LXqfDPwJ2NiXJ25rC89R\nHjasL48qSemV2rmMli2Dk06C5ctjikiSalw9VhnFwuoiSeoZE4IkCUhxQnBQmiT1TGoTgiUESeoZ\nE4IkCTAhSJIiqU0ItiFIUs+kNiFYQpCknknlwLTNm2HoUHjnHdgptSlPkspzYBqwahXstZfJQJJ6\nIpW3TKuLJKnnTAiSJCClCcEeRpLUc6lsVN60KbyGD48xIkmqcT1tVE5lQpAk2ctIktRLJgRJEmBC\nkCRFTAiSJMCEIEmKmBAkSYAJQZIUiTMhTAdWAYvL7JMD5gNLgHyMsUiSuhFnQmgBTiizfTjwK+AL\nwEeAr8QYS2rk8/mkQ6gZXot2Xot2XoveizMhzALWltl+GvA74JVofU2MsaSGX/Z2Xot2Xot2Xove\nS7INYSywG/AIMBf4RoKxSFLm9U/w3AOAw4FPA0OAx4DHgecTjEmSMivuye2agPuAQ0tsuxwYDDRH\n6zcAM4G7Suy7HBjT9+FJUqqtAA5MOoiiJrruZXQw8DDQj1BCWAx8qDphSZI6i7PKaAYwCdgDeBmY\nSqgmApgGPEcoESwC3gN+AzwTYzySJEmS6t0JhJLE84Q2hyx7kVCamg88kWwoVVdqkONuwEPAMuBB\nwriWLCh1LZoJ3bfnR69y43/SZBShl+LThMGtF0WfZ/G70dW1aCYl341+hMbkJkJV0wLgkCQDStgL\nhC96Fn0SmMC2N8GrgR9Ey5cDV1U7qISUuhZTge8lE06iRgKHRcuNwFLCPSKL342urkWPvhu1PJfR\nkYSE8CKwBbgDODnJgGpAvTzytK+VGuT4ReDmaPlmYHJVI0pOVwM+s/jdWEn4oQiwAXgW2Jdsfje6\nuhaQkkdo7ktojC56hfZ/YBYVCL2y5gLnJRxLLRhBqDoheh+RYCy14DvAQuBGslFF0lkToeQ0B78b\nTYRr8Xi0XvF3o5YTQiHpAGrMMYT/yCcCFxKqDhQUyPb35dfA/oQqg1bgmmTDqbpGwjQ4FwNtnbZl\n7bvRSBjLdTGhpNCj70YtJ4T/IzSUFI2ifd6jLGqN3l8D7iFUqWXZKkK9KcDewOoEY0naatpvfDeQ\nre/GAEIyuBW4N/osq9+N4rW4jfZr0aPvRi0nhLmE+Y6agIHA14A/JBlQgoYAw6LlocBnKT+teBb8\nATgzWj6T9v8BsmjvDstfIjvfjQZCNcgzwLUdPs/id6Ora5Gq78aJhNby5cAVCceSpP0JDUYLCF3K\nsnYtZgCvApsJ7UpnE3pcPUy2uhbC9tfiHOAWQpfkhYSbX1bqzCcSBrUuYNtulVn8bpS6FieS3e+G\nJEmSJEmSJEmSJEmSJEmSJPWVDdH7B4Gv9/Gxp3Ra/0sfH1+S1IeKc93kCM/77onunjDYeR4dSVIN\nK960HwfWEUZ0XkyYxuVnhIcPLQTOj/bLEaac/j3hYU0QRnzOJYwaL848exXwbnS8W6PPiqWRhujY\niwmjRk/tcOw8cCdhuuLb+uDfJ0mqUDEhTGLbEsL5wA+j5Q8ATxLm0coRbuwf7LDvrtH7YMJNvrje\nuYRQXP8yYQqFBmAv4O+EyddyhKS0T7Ttr4RZbaVE1PLkdlKcOj805LPAGYRf+I8T5sM5MNr2BOEm\nXnQxYc6Yxwiz8I7t5lwTgdsJM06uBh4FPhatP0GYm6gQHbOpN/8YqS90VycqZcm3Cc/i7SgHvNVp\n/dPAUcAmwnNsB3Vz3ALbJ6DiHP3vdPhsK/4/qQRZQlBWtdE+pTjAn4Bv0X5DPogw7XhnOxMeYbkJ\nOJiQGIq2UPqGPoswfftOwJ7AsYSSQRYfe6ka5q8RZU3xl/lCwi/yBUAL8EtCdc1ThBv1asL88Z2f\nuDUTuIAw7/xSQrVR0X8SGo3nAd/o8Hf3AEdH5ywA34+OfwjbP80rS0/3kiRJkiRJkiRJkiRJkiRJ\nkiRJkiRJ9ez/ASPzoDPHEG8AAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcea2118710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.plot(range(1, RUNS + 1), entropy)\n",
    "2**entropy[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy= [1.6445469704935676, 2.0800064512748428, 2.080006451274842, 2.0800064512748424, 2.1906681946052755, 2.2570115065383876, 2.2786597860645408, 2.2786597860645408, 2.2786597860645408, 2.2786597860645408, 2.2786597860645403, 2.2786597860645408, 2.2786597860645408, 2.2786597860645408, 2.2849509629282276, 2.2849509629282276, 2.2849509629282276, 2.2849509629282272, 2.286874405497795, 2.2868744054977945, 2.2868744054977945, 2.286874405497795, 2.2868744054977945, 2.286874405497795, 2.286874405497795]\n",
      "best_centers= [array([ 2952.76608   ,  1933.02980077,    92.424188  , -2547.74851278,\n",
      "         144.84123959,   154.0172669 ,    18.40817384,     7.84926361,\n",
      "           5.11113863]), array([  428.4738994 ,  1807.58033164,    35.14799298, -2574.43476306,\n",
      "        -180.39839191,   263.09089521,  6048.90511888,  -743.20856056,\n",
      "         256.68319372]), array([ 1492.0570036 ,  1954.30230067,    94.48584365, -2567.99675086,\n",
      "        -112.2682711 ,   152.28015089,   395.84574671,   131.09390181,\n",
      "          73.10315542]), array([  750.10763916,  2067.97627806,    35.34601332, -2398.58742321,\n",
      "        -138.36631381,   233.32209536,  2268.85311051,   245.99611499,\n",
      "         125.46432194]), array([   408.29696084,   1353.92836359,     56.37619358,  -2206.17029272,\n",
      "         -221.37785013,    183.25193705,  18757.57406286,  -5513.4828535 ,\n",
      "         1476.58182765])]\n"
     ]
    }
   ],
   "source": [
    "print 'entropy=',entropy\n",
    "best = np.argmin(cost)\n",
    "print 'best_centers=',list(centroids[best])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
